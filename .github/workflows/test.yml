name: Comprehensive Test Suite

on:
  schedule:
    # Run comprehensive tests daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of tests to run'
        required: true
        default: 'all'
        type: choice
        options:
          - all
          - performance
          - integration
          - security
          - api
      python_version:
        description: 'Python version to test'
        required: false
        default: '3.12'
        type: string

env:
  PYTHON_VERSION: ${{ inputs.python_version || '3.12' }}
  TEST_TYPE: ${{ inputs.test_type || 'all' }}

jobs:
  # Job 1: Performance Tests
  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    if: ${{ contains(github.event.inputs.test_type, 'all') || contains(github.event.inputs.test_type, 'performance') || github.event_name == 'schedule' }}
    timeout-minutes: 30
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-py${{ env.PYTHON_VERSION }}-pip-${{ hashFiles('**/pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-py${{ env.PYTHON_VERSION }}-pip-

      - name: Install dependencies
        run: |
          pip install uv
          uv pip install --system -e ".[dev]"
          pip install pytest-benchmark memory-profiler

      - name: Create test environment file
        run: |
          cp env.example .env
          echo "JUSTIFI_ACCOUNT_ID=test_account" >> .env
          echo "JUSTIFI_API_KEY=test_key" >> .env
          echo "JUSTIFI_ENVIRONMENT=test" >> .env

      - name: Run performance benchmarks
        run: |
          # Create a simple performance test if none exists
          mkdir -p tests/performance
          cat > tests/performance/test_benchmarks.py << 'EOF'
          import pytest
          import asyncio
          from python.core import JustiFiMCPServer
          
          @pytest.mark.benchmark
          def test_server_initialization_performance(benchmark):
              """Test server initialization performance"""
              def init_server():
                  server = JustiFiMCPServer()
                  return server
              
              result = benchmark(init_server)
              assert result is not None
          
          @pytest.mark.benchmark
          def test_tool_loading_performance(benchmark):
              """Test tool loading performance"""
              def load_tools():
                  server = JustiFiMCPServer()
                  return server.get_available_tools()
              
              result = benchmark(load_tools)
              assert len(result) > 0
          EOF
          
          pytest tests/performance/ -v --benchmark-only --benchmark-json=benchmark_results.json

      - name: Memory profiling test
        run: |
          # Create memory profiling test
          cat > memory_test.py << 'EOF'
          import tracemalloc
          from python.core import JustiFiMCPServer
          
          def test_memory_usage():
              tracemalloc.start()
              
              # Initialize server
              server = JustiFiMCPServer()
              
              # Get tools
              tools = server.get_available_tools()
              
              current, peak = tracemalloc.get_traced_memory()
              tracemalloc.stop()
              
              print(f"Current memory usage: {current / 1024 / 1024:.2f} MB")
              print(f"Peak memory usage: {peak / 1024 / 1024:.2f} MB")
              
              # Assert memory usage is reasonable (less than 100MB)
              assert peak < 100 * 1024 * 1024
          
          if __name__ == "__main__":
              test_memory_usage()
          EOF
          
          python memory_test.py

      - name: Upload performance results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-results
          path: |
            benchmark_results.json

  # Job 2: Extended Integration Tests
  extended-integration-tests:
    name: Extended Integration Tests
    runs-on: ubuntu-latest
    if: ${{ contains(github.event.inputs.test_type, 'all') || contains(github.event.inputs.test_type, 'integration') || github.event_name == 'schedule' }}
    timeout-minutes: 20
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          pip install uv
          uv pip install --system -e ".[dev]"

      - name: Create test environment file
        run: |
          cp env.example .env
          echo "JUSTIFI_ACCOUNT_ID=test_account" >> .env
          echo "JUSTIFI_API_KEY=test_key" >> .env
          echo "JUSTIFI_ENVIRONMENT=test" >> .env

      - name: Test MCP server startup and shutdown
        run: |
          # Test server can start and respond
          timeout 30s python -c "
          import asyncio
          import sys
          from python.core import JustiFiMCPServer
          
          async def test_server():
              server = JustiFiMCPServer()
              tools = server.get_available_tools()
              print(f'Server loaded {len(tools)} tools successfully')
              assert len(tools) > 0
              print('✅ Server startup test passed')
          
          asyncio.run(test_server())
          "

      - name: Test tool functionality
        run: |
          # Test each tool can be instantiated
          python -c "
          from python.core import JustiFiMCPServer
          
          server = JustiFiMCPServer()
          tools = server.get_available_tools()
          
          print('Testing tool instantiation...')
          for tool_name in tools:
              try:
                  tool = server.get_tool(tool_name)
                  print(f'✅ {tool_name}: OK')
              except Exception as e:
                  print(f'❌ {tool_name}: {e}')
                  raise
          
          print('✅ All tools instantiated successfully')
          "

      - name: Test configuration loading
        run: |
          # Test configuration with different environments
          python -c "
          import os
          from python.config import Config
          
          # Test default config
          config = Config()
          print(f'Default environment: {config.environment}')
          
          # Test test environment
          os.environ['JUSTIFI_ENVIRONMENT'] = 'test'
          config = Config()
          assert config.environment == 'test'
          print('✅ Configuration loading test passed')
          "

      - name: Test Docker container integration
        run: |
          # Build and test Docker container
          docker build --target development -t justifi-mcp-test:latest .
          
          # Test container can run tools
          docker run --rm --env-file .env justifi-mcp-test:latest python -c "
          from python.core import JustiFiMCPServer
          server = JustiFiMCPServer()
          tools = server.get_available_tools()
          print(f'Container loaded {len(tools)} tools')
          assert len(tools) > 0
          print('✅ Docker integration test passed')
          "

  # Job 3: Security Deep Scan
  security-deep-scan:
    name: Security Deep Scan
    runs-on: ubuntu-latest
    if: ${{ contains(github.event.inputs.test_type, 'all') || contains(github.event.inputs.test_type, 'security') || github.event_name == 'schedule' }}
    timeout-minutes: 15
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install security tools
        run: |
          pip install uv
          uv pip install --system -e ".[dev]"
          pip install bandit safety semgrep

      - name: Run comprehensive security scan
        run: |
          echo "Running Bandit security scan..."
          bandit -r python/ modelcontextprotocol/ -f json -o bandit-detailed.json -v

      - name: Run dependency vulnerability scan
        run: |
          echo "Running Safety dependency scan..."
          safety check --json --output safety-results.json || true

      - name: Run Semgrep security analysis
        run: |
          echo "Running Semgrep security analysis..."
          semgrep --config=auto --json --output=semgrep-results.json python/ modelcontextprotocol/ || true

      - name: Check for sensitive data patterns
        run: |
          echo "Checking for sensitive data patterns..."
          # Check for potential API keys, passwords, etc.
          grep -r -i -n "api_key\|password\|secret\|token" --include="*.py" . || echo "No sensitive patterns found"

      - name: Upload security scan results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: security-deep-scan-results
          path: |
            bandit-detailed.json
            safety-results.json
            semgrep-results.json

  # Job 4: API Compatibility Tests
  api-compatibility-tests:
    name: API Compatibility Tests
    runs-on: ubuntu-latest
    if: ${{ contains(github.event.inputs.test_type, 'all') || contains(github.event.inputs.test_type, 'api') || github.event_name == 'schedule' }}
    timeout-minutes: 10
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          pip install uv
          uv pip install --system -e ".[dev,api]"

      - name: Create test environment file
        run: |
          cp env.example .env
          echo "JUSTIFI_ACCOUNT_ID=test_account" >> .env
          echo "JUSTIFI_API_KEY=test_key" >> .env
          echo "JUSTIFI_ENVIRONMENT=test" >> .env

      - name: Test API schema compatibility
        run: |
          # Run API drift check
          python scripts/ci-drift-check.py || echo "API drift check completed with warnings"

      - name: Test MCP protocol compatibility
        run: |
          # Test MCP protocol implementation
          python -c "
          from python.core import JustiFiMCPServer
          import json
          
          server = JustiFiMCPServer()
          
          # Test that server implements required MCP methods
          required_methods = ['get_available_tools', 'get_tool', 'call_tool']
          
          for method in required_methods:
              assert hasattr(server, method), f'Missing required method: {method}'
              print(f'✅ {method}: OK')
          
          print('✅ MCP protocol compatibility test passed')
          "

  # Job 5: Test Results Summary
  test-summary:
    name: Test Results Summary
    runs-on: ubuntu-latest
    needs: [performance-tests, extended-integration-tests, security-deep-scan, api-compatibility-tests]
    if: always()
    steps:
      - name: Generate comprehensive test summary
        run: |
          echo "## 🧪 Comprehensive Test Suite Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Test Configuration:" >> $GITHUB_STEP_SUMMARY
          echo "- **Python Version**: ${{ env.PYTHON_VERSION }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Test Type**: ${{ env.TEST_TYPE }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Trigger**: ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Test Results:" >> $GITHUB_STEP_SUMMARY
          echo "- **Performance Tests**: ${{ needs.performance-tests.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Extended Integration Tests**: ${{ needs.extended-integration-tests.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Security Deep Scan**: ${{ needs.security-deep-scan.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- **API Compatibility Tests**: ${{ needs.api-compatibility-tests.result }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### 📊 Artifacts Available:" >> $GITHUB_STEP_SUMMARY
          echo "- Performance benchmark results" >> $GITHUB_STEP_SUMMARY
          echo "- Security scan reports" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### 🔍 Recommendations:" >> $GITHUB_STEP_SUMMARY
          if [ "${{ needs.performance-tests.result }}" = "failure" ]; then
            echo "- 🔴 Review performance test failures" >> $GITHUB_STEP_SUMMARY
          fi
          if [ "${{ needs.security-deep-scan.result }}" = "failure" ]; then
            echo "- 🔴 Address security scan findings" >> $GITHUB_STEP_SUMMARY
          fi
          if [ "${{ needs.api-compatibility-tests.result }}" = "failure" ]; then
            echo "- 🔴 Review API compatibility issues" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "---" >> $GITHUB_STEP_SUMMARY
          echo "_Generated by JustiFi MCP Server Comprehensive Test Suite_" >> $GITHUB_STEP_SUMMARY